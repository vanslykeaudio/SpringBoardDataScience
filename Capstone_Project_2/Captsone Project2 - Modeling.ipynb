{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT 2: A COMPUTER VISION MODEL WHICH DETECTS BONE FRACTURES IN THE UPPER EXTREMITIES NAMELY: WRISTS, FOREARMS, UPPER ARM, & SHOULDER FRACTURES\n",
    "#### Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Goal: Build two to three different models and identify the best one. <br>\n",
    "- Fit your models with a training dataset<bf>\n",
    "Hint: Try a number of different models: you will want to compare their outputs in the\n",
    "model evaluation stage. For example, if you’re writing a classification model, you should\n",
    "implement both an entropy model and a Gini impurity model. For hyperparameter tuning,\n",
    "think of methods like cross-validation.<br>\n",
    "- Review model outcomes — Iterate over additional models as needed<br>\n",
    "Hint: you may want to use standard model evaluation metrics such as accuracy, recall,\n",
    "precision, and F1.<br>\n",
    "- Identify the final model that you think is the best model for this project<br>\n",
    "Hint: the most powerful model isn’t always the best one to use. Other considerations\n",
    "include computational complexity, scalability, and maintenance costs.<br>\n",
    "Review the following questions and apply them to your analysis:<br>\n",
    "- Does my data involve a time series or forecasting? If so, am I splitting the train\n",
    "and test data appropriately?<br>\n",
    "- Is my response variable continuous or categorical?<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "# Image Analysis imports\n",
    "import sklearn as skl\n",
    "import skimage as ski\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "from skimage import exposure\n",
    "from skimage.filters import try_all_threshold\n",
    "from skimage.filters import threshold_yen\n",
    "from skimage import measure\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "# PyTorch Analysis imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Feature extraction imports\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data as Pandas DataFrames: df_test, df_train, df_val\n",
    "data_links = [r'C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\df_test.csv', \n",
    "              r'C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\df_train.csv', \n",
    "              r'C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\df_val.csv']\n",
    "\n",
    "def multi_df(list):\n",
    "    temp_list = []\n",
    "    for i in list:\n",
    "        temp_list.append(pd.read_csv(i))\n",
    "    return temp_list\n",
    "    \n",
    "df_test, df_train, df_val = multi_df(data_links)\n",
    "\n",
    "# Trim the extra index column\n",
    "df_train.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_test.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_val.drop(columns='Unnamed: 0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "# Project Main Directory path\n",
    "dir_path = r\"C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\"\n",
    "\n",
    "# Data Paths\n",
    "train_dir_path = r\"C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\train\"\n",
    "val_dir_path = r\"C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\valid\"\n",
    "test_dir_path = r\"C:\\Users\\micha\\OneDrive\\Documents\\GitHub\\SpringBoardDataScience\\Capstone_Project_2\\Data\\test\"\n",
    "\n",
    "# Image Scalars\n",
    "Width = 400\n",
    "Height = 400\n",
    "\n",
    "# Labels\n",
    "num_classes = 8\n",
    "classes=['elbow positive', 'fingers positive', 'forearm fracture', 'humerus fracture', 'humerus', 'shoulder fracture', 'wrist positive', 'no fracture']\n",
    "c2l={k:v for k,v in list(zip(classes,list(range(num_classes))))}\n",
    "l2c={v:k for k,v in c2l.items()}\n",
    "\n",
    "# Training\n",
    "BS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models: <br>\n",
    "We are going to train 3 models: The first being a basic Resnet model, which we will train without using and pretrained data on the model, and then we will do another Resnet Model using pretrained data.  Also as a fun exercise we will include a model which uses segmentation w/bounding boxes rather to see if we achieve a better performance, therefore we will be building a UNet() model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Declare Dataset (train/validate) and DataLoader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "def import_boxes(list):\n",
    "    '''\n",
    "    Function for converting the text data in the labels files into functional\n",
    "    dictionaries:  Given a list from a read file it converts that list and \n",
    "    returns a dictionary with correlated label and coordinate data.\n",
    "    '''\n",
    "    # Convert strings to ints and floats\n",
    "    for i in range(0, len(list)):\n",
    "        if len(list[i]) == 1:\n",
    "            list[i] = int(float(list[i]))\n",
    "        else:\n",
    "            list[i] = float(list[i])\n",
    "    # initialize variables        \n",
    "    boxes = {'labels': [],\n",
    "              'coords': []}\n",
    "    i = -1\n",
    "    neg_len = len(list) - (len(list) * 2)\n",
    "    temp_list = []\n",
    "    # Convert list to a functional dictionary of labels and coords\n",
    "    while i >=  neg_len:\n",
    "        if type(list[i]) == int:\n",
    "            boxes['labels'].append(list[i])\n",
    "            boxes['coords'].append(temp_list)\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.insert(0, list[i])\n",
    "        i -= 1\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def display_features(img, return_nodes, out, tensor, tensor_index):\n",
    "    \n",
    "    _, ax = plt.subplots(4, 5, figsize=(25,20))\n",
    "\n",
    "    for i, layer in enumerate(return_nodes):\n",
    "        feat_maps = out[layer].numpy().squeeze(0)\n",
    "        feat_maps = sample(list(feat_maps), 4)\n",
    "        labeled_image = draw_bounding_boxes(img, boxes=tensor.get_boxes(tensor_index), colors=\"red\", labels=[str(x) for x in tensor.get_labels(tensor_index)])\n",
    "        ax[i][0].imshow(labeled_image.permute(1, 2, 0))\n",
    "        ax[i][0].set_xticks([])\n",
    "        ax[i][0].set_yticks([])\n",
    "        for j, feat_map in enumerate(feat_maps):\n",
    "            sns.heatmap(feat_map, ax=ax[i][j+1], cbar=False)\n",
    "            ax[i][j+1].set_xticks([])\n",
    "            ax[i][j+1].set_yticks([])\n",
    "            ax[i][j+1].set_title(f\"{layer}: ({feat_map.shape[0]} X {feat_map.shape[1]})\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.files = [rf\"{root}\\Data\\{mode}\\images\\{name}\" for name in os.listdir(root + rf\"\\Data\\{mode}\\images\") if name.endswith(\".jpg\")]\n",
    "        self.labels = {}\n",
    "        self.boxes = {}\n",
    "        assert len(self.files) > 0, \"Make sure you downloaded the images!\"\n",
    "\n",
    "    def get_image(self, index):\n",
    "        item = self.transform(Image.open(self.files[index % len(self.files)]))\n",
    "        if item.shape[0] != 3: \n",
    "            item = item.repeat(3, 1, 1)\n",
    "        # Old versions of PyTorch didn't support normalization for different-channeled images\n",
    "        item_mean, item_std = item.mean([1,2]), item.std([1,2])\n",
    "        normalize = transforms.Compose([\n",
    "            v2.RandomAutocontrast(p=1.0),\n",
    "            transforms.Normalize(item_mean, item_std),\n",
    "        ])\n",
    "        item = normalize(item)\n",
    "        #item = np.array(item)\n",
    "        return item\n",
    "    \n",
    "    def get_labels(self, index):\n",
    "        if index in self.labels:\n",
    "            return self.labels[index]\n",
    "        labels = []\n",
    "        boxes = []\n",
    "        text_file = self.files[index % len(self.files)].replace(\".jpg\", \".txt\").replace(\"images\", \"labels\")\n",
    "        with open(text_file, mode=\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                values = [value for value in line.split()]\n",
    "                bboxes = import_boxes(values)\n",
    "                for i in range(0, len(bboxes['labels'])):\n",
    "                    coords = bboxes['coords'][i]\n",
    "                    coords = torch.reshape(torch.FloatTensor(coords), (int(len(coords)/2), 2))\n",
    "                    size = torch.tensor([self.get_image(index).shape[2], self.get_image(index).shape[1]])\n",
    "                    min_coords = torch.min(coords, dim=0).values * size\n",
    "                    max_coords = torch.max(coords, dim=0).values * size\n",
    "                    box = torch.cat((min_coords, max_coords), dim=0).tolist()\n",
    "                    labels.append(l2c[bboxes['labels'][i]])\n",
    "                    boxes.append(box)\n",
    "        if not labels:\n",
    "            labels.append('no fracture')\n",
    "            self.labels[index] = labels\n",
    "            self.boxes[index] = torch.FloatTensor([[0,0,0,0]])\n",
    "        else:\n",
    "            self.labels[index] = labels\n",
    "            self.boxes[index] = torch.FloatTensor(boxes)\n",
    "        return labels\n",
    "    \n",
    "    def get_boxes(self, index):\n",
    "        if index in self.boxes:\n",
    "            return self.boxes[index]\n",
    "        _ = self.get_labels(index)\n",
    "        return self.boxes[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
