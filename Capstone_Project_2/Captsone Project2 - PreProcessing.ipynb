{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT 2: A COMPUTER VISION MODEL WHICH DETECTS BONE FRACTURES IN THE UPPER EXTREMITIES NAMELY: WRISTS, FOREARMS, UPPER ARM, & SHOULDER FRACTURES\n",
    "#### Data Pre-Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Goal: Create a cleaned development dataset you can use to complete the modeling step of your project. <br>\n",
    "This is going to be unlike regular preprocessing:\n",
    "- first of all, I have already a separated training, testing and validation data sets.\n",
    "- secondly, since we are using PyTorch for the image evaluation and training, the data is in the image Tensors. In the EDA phase of the project we developed tools for accessing converting and even visualizing to a degree the patterns that can be found in the images.\n",
    "- Therefore there will be a good deal of code which will copy and pasted from the last project inorder to get us started, but a lot of this will be focused on image prep before feeding to the model to improve it's functionality.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "# Image Analysis imports\n",
    "import sklearn as skl\n",
    "import skimage as ski\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "from skimage import exposure\n",
    "from skimage.filters import try_all_threshold\n",
    "from skimage.filters import threshold_yen\n",
    "from skimage import measure\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "# PyTorch Analysis imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Feature extraction imports\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived from my Explorations during EDA, I believe that simply enhancing images will be sufficient to improve model performance when \"seeing\" into the image to detect fractures.\n",
    "after that I think any other transformations of the image data will be superfluous.  Therefore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
